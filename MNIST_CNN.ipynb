{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # of GPUs available :  4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "%matplotlib inline\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']=\"0,1\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(' # of GPUs available : ', torch.cuda.device_count())\n",
    "\n",
    "log_interval = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR  = './results/'\n",
    "\n",
    "# Input info\n",
    "INPUT_DIR  = '../Data/'\n",
    "INPUT_DIR  = '/input/AIRLab/PaintInsReport/Output'\n",
    "INPUT_FILE = 'insp_data_new'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16,\n",
    "                                         shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Dataset size :  torch.Size([60000, 28, 28])\n",
      " Training Label size   :  torch.Size([60000])\n",
      " Testing Dataset size :  torch.Size([10000, 28, 28])\n",
      " Testing Label size   :  torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(' Training Dataset size : ', trainloader.dataset.train_data.shape)\n",
    "print(' Training Label size   : ', trainloader.dataset.train_labels.shape)\n",
    "\n",
    "print(' Testing Dataset size : ', testloader.dataset.test_data.shape)\n",
    "print(' Testing Label size   : ', testloader.dataset.test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    6     9     3     8     7     5     2     5     7     8     4     9     2     4     1     0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % labels[j].item() for j in range(16)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "        \n",
    "#         # Layer 1\n",
    "#         self.conv1 = nn.Conv2d(1, 64, 3) # 28x28 -> 26x26\n",
    "#         self.b1    = nn.BatchNorm2d(64)\n",
    "#         self.pool  = nn.MaxPool2d(2, 2)  # 26x26 -> 13x13\n",
    "        \n",
    "#         # Layer 2\n",
    "#         self.conv2 = nn.Conv2d(64, 128, 3)  # 13x13 -> 11x11\n",
    "#         self.b2    = nn.BatchNorm2d(128)\n",
    "#         #self.pool                          # 11x11 -> 5x5\n",
    "        \n",
    "#         # Layer 3\n",
    "#         self.conv3 = nn.Conv2d(128, 128, 3) # 5x5 -> 3x3\n",
    "#         self.b3    = nn.BatchNorm2d(128)\n",
    "#         #self.pool                          # 3x3 -> 1x1 \n",
    "        \n",
    "#         # FC Layers\n",
    "#         self.fc1 = nn.Linear(128 * 1 * 1, 512)\n",
    "#         self.bf1 = nn.BatchNorm1d(512)\n",
    "        \n",
    "#         self.fc2 = nn.Linear(512, 256)\n",
    "#         self.bf2 = nn.BatchNorm1d(256)\n",
    "        \n",
    "#         self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Layer 1\n",
    "#         x = self.pool(F.relu(self.b1(self.conv1(x))))\n",
    "#         #print(self.num_flat_features(x))\n",
    "        \n",
    "#         # Layer 2\n",
    "#         x = self.pool(F.relu(self.b2(self.conv2(x))))\n",
    "        \n",
    "#         # Layer 3\n",
    "#         x = self.pool(F.relu(self.b3(self.conv3(x))))\n",
    "        \n",
    "#         # Flatten tensors\n",
    "#         x = x.view(-1, self.num_flat_features(x))\n",
    "        \n",
    "#         # FC Layer 1\n",
    "#         x = F.relu(self.bf1(self.fc1(x)))\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "        \n",
    "#         # FC Layer 2\n",
    "#         x = F.relu(self.bf2(self.fc2(x)))\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "        \n",
    "#         # FC Layer 3\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "    \n",
    "    \n",
    "#     def num_flat_features(self, x):\n",
    "#         size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "#         num_features = 1\n",
    "#         for s in size:\n",
    "#             num_features *= s\n",
    "#         return num_features\n",
    "\n",
    "# net = Net()\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#     # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "#     net = nn.DataParallel(net)\n",
    "\n",
    "# net.to(device)\n",
    "\n",
    "# print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 4 GPUs!\n",
      "DataParallel(\n",
      "  (module): Net(\n",
      "    (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (b1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (b2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (b3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "    (bf1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (bf2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3) # 28x28 -> 26x26  /40x40 -> 38x38 / 60x60 -> 58x58 / 80x80 -> 78x78 / 100x100 -> 98x98\n",
    "        self.b1    = nn.BatchNorm2d(16)\n",
    "        self.pool  = nn.MaxPool2d(2, 2)  # 26x26 -> 13x13  /38x38 -> 19x19 / 58x58 -> 29x29 / 78x78 -> 39x39 / 98x98 -> 49x49\n",
    "        \n",
    "        # Layer 2\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)  # 13x13 -> 11x11 /19x19 -> 17x17 / 29x29 -> 27x27 / 39x39 -> 37x37 / 49x49 -> 47x47\n",
    "        self.b2    = nn.BatchNorm2d(32)\n",
    "        #self.pool                          # 11x11 -> 5x5  /17x17 -> 8x8 / 27x27 -> 13x13 / 37x37 -> 18x18 / 47x47 -> 23x23\n",
    "        \n",
    "        # Layer 3\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3) # 5x5 -> 3x3  / 8x8 -> 6x6 / 13x13 -> 11x11 / 18x18 -> 16x16 / 23x23 -> 21x21\n",
    "        self.b3    = nn.BatchNorm2d(64)\n",
    "        #self.pool                          # 3x3 -> 1x1  /6x6 -> 3x3 / 11x11 -> 5x5 / 16x16 -> 8x8 / 21x21 -> 10x10\n",
    "        \n",
    "        # FC Layers\n",
    "        self.fc1 = nn.Linear(64 * 1 * 1, 256)\n",
    "        self.bf1 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bf2 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "        # Layer 1\n",
    "        x = self.pool(F.relu(self.b1(self.conv1(x))))\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.pool(F.relu(self.b2(self.conv2(x))))\n",
    "        \n",
    "        # Layer 3\n",
    "        x = self.pool(F.relu(self.b3(self.conv3(x))))\n",
    "        \n",
    "        # Flatten tensors\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        \n",
    "        # FC Layer 1\n",
    "        x = F.relu(self.bf1(self.fc1(x)))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        # FC Layer 2\n",
    "        x = F.relu(self.bf2(self.fc2(x)))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        # FC Layer 3\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net = Net()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "    net = nn.DataParallel(net)\n",
    "\n",
    "net.to(device)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    net.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0.0\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # For Multi-gpu processing\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        net.train(True) # Set to Train mode\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        # Calculate Loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate Training Accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()    \n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % log_interval == 0:    # print every 200 mini-batches\n",
    "            print('Train Epoch : %2d [%6d, %6d] loss: %.3f TrnAcc: %.3f' %\n",
    "                  (epoch, total, len(trainloader.dataset), running_loss / 2000, correct / total))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    net.eval()\n",
    "    # Calculate Validation Accuracy\n",
    "    total_eval = 0\n",
    "    correct_eval = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images_eval, labels_eval in testloader:\n",
    "            images_eval, labels_eval = images_eval.to(device), labels_eval.to(device)\n",
    "            outputs_eval = net(images_eval)\n",
    "            _, predicted_eval = torch.max(outputs_eval.data, 1)\n",
    "            total_eval += labels_eval.size(0)\n",
    "            correct_eval += (predicted_eval == labels_eval).sum().item() \n",
    "    val_acc = correct_eval / total_eval\n",
    "    print('Test | Size : %6d  | ValAcc: %.3f\\n' %\n",
    "          (len(testloader.dataset), val_acc))\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | Size :  10000  | ValAcc: 0.098\n",
      "\n",
      "Train Epoch :  0 [    16,  60000] loss: 0.001 TrnAcc: 0.125\n",
      "Train Epoch :  0 [  8016,  60000] loss: 0.336 TrnAcc: 0.594\n",
      "Train Epoch :  0 [ 16016,  60000] loss: 0.190 TrnAcc: 0.686\n",
      "Train Epoch :  0 [ 24016,  60000] loss: 0.147 TrnAcc: 0.734\n",
      "Train Epoch :  0 [ 32016,  60000] loss: 0.130 TrnAcc: 0.762\n",
      "Train Epoch :  0 [ 40016,  60000] loss: 0.120 TrnAcc: 0.781\n",
      "Train Epoch :  0 [ 48016,  60000] loss: 0.118 TrnAcc: 0.795\n",
      "Train Epoch :  0 [ 56016,  60000] loss: 0.111 TrnAcc: 0.807\n",
      "Test | Size :  10000  | ValAcc: 0.969\n",
      "\n",
      "\u001b[31m BEST EPOCH UPDATED!! [0] \u001b[0m\n",
      "Train Epoch :  1 [    16,  60000] loss: 0.000 TrnAcc: 0.875\n",
      "Train Epoch :  1 [  8016,  60000] loss: 0.100 TrnAcc: 0.890\n",
      "Train Epoch :  1 [ 16016,  60000] loss: 0.096 TrnAcc: 0.891\n",
      "Train Epoch :  1 [ 24016,  60000] loss: 0.098 TrnAcc: 0.891\n",
      "Train Epoch :  1 [ 32016,  60000] loss: 0.092 TrnAcc: 0.892\n",
      "Train Epoch :  1 [ 40016,  60000] loss: 0.091 TrnAcc: 0.893\n",
      "Train Epoch :  1 [ 48016,  60000] loss: 0.089 TrnAcc: 0.894\n",
      "Train Epoch :  1 [ 56016,  60000] loss: 0.086 TrnAcc: 0.896\n",
      "Test | Size :  10000  | ValAcc: 0.977\n",
      "\n",
      "\u001b[31m BEST EPOCH UPDATED!! [1] \u001b[0m\n",
      "Train Epoch :  2 [    16,  60000] loss: 0.000 TrnAcc: 0.938\n",
      "Train Epoch :  2 [  8016,  60000] loss: 0.084 TrnAcc: 0.909\n",
      "Train Epoch :  2 [ 16016,  60000] loss: 0.081 TrnAcc: 0.909\n",
      "Train Epoch :  2 [ 24016,  60000] loss: 0.080 TrnAcc: 0.910\n",
      "Train Epoch :  2 [ 32016,  60000] loss: 0.079 TrnAcc: 0.911\n",
      "Train Epoch :  2 [ 40016,  60000] loss: 0.079 TrnAcc: 0.911\n",
      "Train Epoch :  2 [ 48016,  60000] loss: 0.073 TrnAcc: 0.913\n",
      "Train Epoch :  2 [ 56016,  60000] loss: 0.075 TrnAcc: 0.914\n",
      "Test | Size :  10000  | ValAcc: 0.980\n",
      "\n",
      "\u001b[31m BEST EPOCH UPDATED!! [2] \u001b[0m\n",
      "Train Epoch :  3 [    16,  60000] loss: 0.000 TrnAcc: 0.875\n",
      "Train Epoch :  3 [  8016,  60000] loss: 0.074 TrnAcc: 0.915\n",
      "Train Epoch :  3 [ 16016,  60000] loss: 0.077 TrnAcc: 0.917\n",
      "Train Epoch :  3 [ 24016,  60000] loss: 0.068 TrnAcc: 0.920\n",
      "Train Epoch :  3 [ 32016,  60000] loss: 0.069 TrnAcc: 0.921\n",
      "Train Epoch :  3 [ 40016,  60000] loss: 0.075 TrnAcc: 0.921\n",
      "Train Epoch :  3 [ 48016,  60000] loss: 0.074 TrnAcc: 0.920\n",
      "Train Epoch :  3 [ 56016,  60000] loss: 0.066 TrnAcc: 0.921\n",
      "Test | Size :  10000  | ValAcc: 0.981\n",
      "\n",
      "\u001b[31m BEST EPOCH UPDATED!! [3] \u001b[0m\n",
      "Train Epoch :  4 [    16,  60000] loss: 0.000 TrnAcc: 0.938\n",
      "Train Epoch :  4 [  8016,  60000] loss: 0.062 TrnAcc: 0.932\n",
      "Train Epoch :  4 [ 16016,  60000] loss: 0.070 TrnAcc: 0.931\n",
      "Train Epoch :  4 [ 24016,  60000] loss: 0.067 TrnAcc: 0.930\n",
      "Train Epoch :  4 [ 32016,  60000] loss: 0.070 TrnAcc: 0.928\n",
      "Train Epoch :  4 [ 40016,  60000] loss: 0.062 TrnAcc: 0.929\n",
      "Train Epoch :  4 [ 48016,  60000] loss: 0.070 TrnAcc: 0.928\n",
      "Train Epoch :  4 [ 56016,  60000] loss: 0.065 TrnAcc: 0.928\n",
      "Test | Size :  10000  | ValAcc: 0.984\n",
      "\n",
      "\u001b[31m BEST EPOCH UPDATED!! [4] \u001b[0m\n",
      "Train Epoch :  5 [    16,  60000] loss: 0.000 TrnAcc: 0.938\n",
      "Train Epoch :  5 [  8016,  60000] loss: 0.064 TrnAcc: 0.935\n",
      "Train Epoch :  5 [ 16016,  60000] loss: 0.063 TrnAcc: 0.934\n",
      "Train Epoch :  5 [ 24016,  60000] loss: 0.061 TrnAcc: 0.934\n",
      "Train Epoch :  5 [ 32016,  60000] loss: 0.066 TrnAcc: 0.933\n",
      "Train Epoch :  5 [ 40016,  60000] loss: 0.058 TrnAcc: 0.933\n",
      "Train Epoch :  5 [ 48016,  60000] loss: 0.059 TrnAcc: 0.934\n",
      "Train Epoch :  5 [ 56016,  60000] loss: 0.068 TrnAcc: 0.933\n",
      "Test | Size :  10000  | ValAcc: 0.985\n",
      "\n",
      "\u001b[31m BEST EPOCH UPDATED!! [5] \u001b[0m\n",
      "Train Epoch :  6 [    16,  60000] loss: 0.000 TrnAcc: 1.000\n",
      "Train Epoch :  6 [  8016,  60000] loss: 0.062 TrnAcc: 0.933\n",
      "Train Epoch :  6 [ 16016,  60000] loss: 0.058 TrnAcc: 0.935\n",
      "Train Epoch :  6 [ 24016,  60000] loss: 0.061 TrnAcc: 0.934\n",
      "Train Epoch :  6 [ 32016,  60000] loss: 0.059 TrnAcc: 0.934\n",
      "Train Epoch :  6 [ 40016,  60000] loss: 0.063 TrnAcc: 0.934\n",
      "Train Epoch :  6 [ 48016,  60000] loss: 0.064 TrnAcc: 0.934\n",
      "Train Epoch :  6 [ 56016,  60000] loss: 0.060 TrnAcc: 0.934\n",
      "Test | Size :  10000  | ValAcc: 0.986\n",
      "\n",
      "\u001b[31m BEST EPOCH UPDATED!! [6] \u001b[0m\n",
      "Train Epoch :  7 [    16,  60000] loss: 0.000 TrnAcc: 1.000\n",
      "Train Epoch :  7 [  8016,  60000] loss: 0.060 TrnAcc: 0.936\n",
      "Train Epoch :  7 [ 16016,  60000] loss: 0.058 TrnAcc: 0.937\n",
      "Train Epoch :  7 [ 24016,  60000] loss: 0.056 TrnAcc: 0.938\n",
      "Train Epoch :  7 [ 32016,  60000] loss: 0.056 TrnAcc: 0.938\n",
      "Train Epoch :  7 [ 40016,  60000] loss: 0.055 TrnAcc: 0.938\n",
      "Train Epoch :  7 [ 48016,  60000] loss: 0.055 TrnAcc: 0.938\n",
      "Train Epoch :  7 [ 56016,  60000] loss: 0.062 TrnAcc: 0.937\n",
      "Test | Size :  10000  | ValAcc: 0.986\n",
      "\n",
      "Train Epoch :  8 [    16,  60000] loss: 0.000 TrnAcc: 0.938\n",
      "Train Epoch :  8 [  8016,  60000] loss: 0.054 TrnAcc: 0.941\n",
      "Train Epoch :  8 [ 16016,  60000] loss: 0.056 TrnAcc: 0.940\n",
      "Train Epoch :  8 [ 24016,  60000] loss: 0.053 TrnAcc: 0.940\n",
      "Train Epoch :  8 [ 32016,  60000] loss: 0.059 TrnAcc: 0.940\n",
      "Train Epoch :  8 [ 40016,  60000] loss: 0.055 TrnAcc: 0.940\n",
      "Train Epoch :  8 [ 48016,  60000] loss: 0.057 TrnAcc: 0.941\n",
      "Train Epoch :  8 [ 56016,  60000] loss: 0.059 TrnAcc: 0.940\n",
      "Test | Size :  10000  | ValAcc: 0.988\n",
      "\n",
      "\u001b[31m BEST EPOCH UPDATED!! [8] \u001b[0m\n",
      "Train Epoch :  9 [    16,  60000] loss: 0.000 TrnAcc: 1.000\n",
      "Train Epoch :  9 [  8016,  60000] loss: 0.053 TrnAcc: 0.944\n",
      "Train Epoch :  9 [ 16016,  60000] loss: 0.061 TrnAcc: 0.942\n",
      "Train Epoch :  9 [ 24016,  60000] loss: 0.053 TrnAcc: 0.942\n",
      "Train Epoch :  9 [ 32016,  60000] loss: 0.055 TrnAcc: 0.942\n",
      "Train Epoch :  9 [ 40016,  60000] loss: 0.053 TrnAcc: 0.942\n",
      "Train Epoch :  9 [ 48016,  60000] loss: 0.052 TrnAcc: 0.942\n",
      "Train Epoch :  9 [ 56016,  60000] loss: 0.056 TrnAcc: 0.942\n",
      "Test | Size :  10000  | ValAcc: 0.986\n",
      "\n",
      "Train Epoch : 10 [    16,  60000] loss: 0.000 TrnAcc: 0.875\n",
      "Train Epoch : 10 [  8016,  60000] loss: 0.055 TrnAcc: 0.942\n",
      "Train Epoch : 10 [ 16016,  60000] loss: 0.047 TrnAcc: 0.945\n",
      "Train Epoch : 10 [ 24016,  60000] loss: 0.057 TrnAcc: 0.944\n",
      "Train Epoch : 10 [ 32016,  60000] loss: 0.055 TrnAcc: 0.944\n",
      "Train Epoch : 10 [ 40016,  60000] loss: 0.046 TrnAcc: 0.946\n",
      "Train Epoch : 10 [ 48016,  60000] loss: 0.050 TrnAcc: 0.946\n",
      "Train Epoch : 10 [ 56016,  60000] loss: 0.059 TrnAcc: 0.945\n",
      "Test | Size :  10000  | ValAcc: 0.984\n",
      "\n",
      "Train Epoch : 11 [    16,  60000] loss: 0.000 TrnAcc: 0.938\n",
      "Train Epoch : 11 [  8016,  60000] loss: 0.055 TrnAcc: 0.944\n",
      "Train Epoch : 11 [ 16016,  60000] loss: 0.049 TrnAcc: 0.945\n",
      "Train Epoch : 11 [ 24016,  60000] loss: 0.054 TrnAcc: 0.943\n",
      "Train Epoch : 11 [ 32016,  60000] loss: 0.055 TrnAcc: 0.943\n",
      "Train Epoch : 11 [ 40016,  60000] loss: 0.050 TrnAcc: 0.943\n",
      "Train Epoch : 11 [ 48016,  60000] loss: 0.043 TrnAcc: 0.945\n",
      "Train Epoch : 11 [ 56016,  60000] loss: 0.049 TrnAcc: 0.946\n",
      "Test | Size :  10000  | ValAcc: 0.987\n",
      "\n",
      "Train Epoch : 12 [    16,  60000] loss: 0.000 TrnAcc: 1.000\n",
      "Train Epoch : 12 [  8016,  60000] loss: 0.049 TrnAcc: 0.947\n",
      "Train Epoch : 12 [ 16016,  60000] loss: 0.050 TrnAcc: 0.947\n",
      "Train Epoch : 12 [ 24016,  60000] loss: 0.050 TrnAcc: 0.946\n",
      "Train Epoch : 12 [ 32016,  60000] loss: 0.049 TrnAcc: 0.947\n",
      "Train Epoch : 12 [ 40016,  60000] loss: 0.048 TrnAcc: 0.947\n",
      "Train Epoch : 12 [ 48016,  60000] loss: 0.049 TrnAcc: 0.947\n",
      "Train Epoch : 12 [ 56016,  60000] loss: 0.045 TrnAcc: 0.948\n",
      "Test | Size :  10000  | ValAcc: 0.986\n",
      "\n",
      "Train Epoch : 13 [    16,  60000] loss: 0.000 TrnAcc: 1.000\n",
      "Train Epoch : 13 [  8016,  60000] loss: 0.051 TrnAcc: 0.945\n",
      "Train Epoch : 13 [ 16016,  60000] loss: 0.048 TrnAcc: 0.946\n",
      "Train Epoch : 13 [ 24016,  60000] loss: 0.049 TrnAcc: 0.947\n",
      "Train Epoch : 13 [ 32016,  60000] loss: 0.046 TrnAcc: 0.947\n",
      "Train Epoch : 13 [ 40016,  60000] loss: 0.046 TrnAcc: 0.948\n",
      "Train Epoch : 13 [ 48016,  60000] loss: 0.053 TrnAcc: 0.947\n",
      "Train Epoch : 13 [ 56016,  60000] loss: 0.045 TrnAcc: 0.948\n",
      "Test | Size :  10000  | ValAcc: 0.985\n",
      "\n",
      "Train Epoch : 14 [    16,  60000] loss: 0.000 TrnAcc: 1.000\n",
      "Train Epoch : 14 [  8016,  60000] loss: 0.047 TrnAcc: 0.951\n",
      "Train Epoch : 14 [ 16016,  60000] loss: 0.048 TrnAcc: 0.949\n",
      "Train Epoch : 14 [ 24016,  60000] loss: 0.047 TrnAcc: 0.951\n",
      "Train Epoch : 14 [ 32016,  60000] loss: 0.049 TrnAcc: 0.950\n",
      "Train Epoch : 14 [ 40016,  60000] loss: 0.044 TrnAcc: 0.950\n",
      "Train Epoch : 14 [ 48016,  60000] loss: 0.045 TrnAcc: 0.950\n",
      "Train Epoch : 14 [ 56016,  60000] loss: 0.051 TrnAcc: 0.950\n",
      "Test | Size :  10000  | ValAcc: 0.985\n",
      "\n",
      "Train Epoch : 15 [    16,  60000] loss: 0.000 TrnAcc: 0.938\n",
      "Train Epoch : 15 [  8016,  60000] loss: 0.044 TrnAcc: 0.954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch : 15 [ 16016,  60000] loss: 0.046 TrnAcc: 0.952\n",
      "Train Epoch : 15 [ 24016,  60000] loss: 0.048 TrnAcc: 0.951\n",
      "Train Epoch : 15 [ 32016,  60000] loss: 0.046 TrnAcc: 0.951\n",
      "Train Epoch : 15 [ 40016,  60000] loss: 0.052 TrnAcc: 0.950\n",
      "Train Epoch : 15 [ 48016,  60000] loss: 0.050 TrnAcc: 0.950\n",
      "Train Epoch : 15 [ 56016,  60000] loss: 0.046 TrnAcc: 0.950\n",
      "Test | Size :  10000  | ValAcc: 0.985\n",
      "\n",
      "Train Epoch : 16 [    16,  60000] loss: 0.000 TrnAcc: 1.000\n",
      "Train Epoch : 16 [  8016,  60000] loss: 0.046 TrnAcc: 0.948\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f1b83a5c45d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# loop over the dataset multiple times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_val_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-82e2007cac69>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test() # Show initial performance\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "    train(epoch)\n",
    "    val_acc = test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch\n",
    "        # Save model\n",
    "        net.train()\n",
    "#         torch.save(net.module.state_dict(), './results/model_190314.pth')\n",
    "#         torch.save(optimizer.state_dict(), './results/optimizer_190314.pth')\n",
    "        torch.save({\n",
    "                    'epoch': best_epoch,\n",
    "                    'model_state_dict': net.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'val_acc': best_val_acc\n",
    "                    }, \n",
    "                    './results/model_190314.pth')        \n",
    "        \n",
    "        print (\"\\x1b[31m BEST EPOCH UPDATED!! [%d] \\x1b[0m\" % (best_epoch))\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in net.state_dict():\n",
    "    print(param_tensor, \"\\t\", net.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Epoch :  4 [ 56016,  60000] loss: 0.051 TrnAcc: 0.940\n",
    "Test | Size :  10000  | ValAcc: 0.987"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refs\n",
    "https://www.kaggle.com/sdelecourt/cnn-with-pytorch-for-mnist\n",
    "\n",
    "https://nextjournal.com/gkoehler/pytorch-mnist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
